#+TITLE: Langchain Expression Languageï¼ˆLCELï¼‰å¿«é€Ÿå…¥é—¨
#+STARTUP: showall hidestars indent inlineimages
#+PROPERTY: header-args:jupyter-python :session 2024äººå·¥æ™ºèƒ½å­¦ä¹ -LangChain-LCEL-quickstart :display text/plain

* Langchain Expression Languageï¼ˆLCELï¼‰å¿«é€Ÿå…¥é—¨
*LCELæ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾å¼ç»„åˆè¯­è¨€* ï¼Œèƒ½å¤Ÿä»ç®€å•çš„=Prompt+LLM=é“¾åˆ°å¤æ‚çš„é“¾ï¼ˆè¿è¡Œ100å¤šä¸ªæ­¥éª¤çš„ç”Ÿäº§ç¯å¢ƒï¼‰ã€‚

å…¶è®¾è®¡åˆè¡·æ˜¯æ”¯æŒå°†åŸå‹å¿«é€ŸæŠ•å…¥ç”Ÿäº§ï¼Œè€Œæ— éœ€æ›´æ”¹ä»£ç ã€‚

LCELä½¿å¾—ä»åŸºæœ¬ç»„ä»¶æ„å»ºå¤æ‚é“¾å˜å¾—å®¹æ˜“ï¼Œå¹¶æ”¯æŒæµå¼å¤„ç†ã€å¹¶è¡Œå¤„ç†å’Œæ—¥å¿—è®°å½•ç­‰å¼€ç®±å³ç”¨çš„åŠŸèƒ½ã€‚

** ç»„ä»¶
æˆ‘ä»¬å·²å­¦ä¹ çš„ç»„ä»¶åŒ…æ‹¬ä»¥ä¸‹æ¨¡å—ï¼š

*** ğŸ“ƒ Model I/Oï¼š
è¿™åŒ…æ‹¬æç¤ºç®¡ç†ï¼Œæç¤ºä¼˜åŒ–ï¼Œç”¨äºèŠå¤©æ¨¡å‹å’ŒLLMçš„é€šç”¨æ¥å£ï¼Œä»¥åŠå¤„ç†æ¨¡å‹è¾“å‡ºçš„å¸¸è§å®ç”¨å·¥å…·ã€‚

*** ğŸ“š Retrievalï¼š
æ£€ç´¢å¢å¼ºç”Ÿæˆæ¶‰åŠä»å„ç§æ¥æºåŠ è½½æ•°æ®ï¼Œå‡†å¤‡æ•°æ®ï¼Œç„¶ååœ¨ç”Ÿæˆæ­¥éª¤ä¸­æ£€ç´¢æ•°æ®ä»¥ä¾›ä½¿ç”¨ã€‚

*** ğŸ¤– Agentsï¼š
Agents å…è®¸LLMè‡ªä¸»å®Œæˆä»»åŠ¡ã€‚
Agentsä¼šå†³å®šé‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Œç„¶åæ‰§è¡Œè¯¥è¡ŒåŠ¨ï¼Œå¹¶è§‚å¯Ÿç»“æœï¼Œå¹¶é‡å¤æ­¤è¿‡ç¨‹ç›´åˆ°ä»»åŠ¡å®Œæˆã€‚
LangChainä¸ºä»£ç†æä¾›äº†æ ‡å‡†æ¥å£ã€å¯é€‰æ‹©çš„ä»£ç†åˆ—è¡¨ä»¥åŠç«¯åˆ°ç«¯ä»£ç†ç¤ºä¾‹ã€‚

** ä½¿ç”¨ LCEL å®ç° LLMChainï¼ˆPrompt + LLM)
*** Pipe ç®¡é“æ“ä½œç¬¦
æˆ‘ä»¬ä½¿ç”¨LCELçš„ =|= æ“ä½œç¬¦å°†è¿™äº›ä¸åŒç»„ä»¶æ‹¼æ¥æˆä¸€ä¸ªå•ä¸€é“¾ã€‚

*åœ¨è¿™ä¸ªé“¾ä¸­ï¼Œç”¨æˆ·è¾“å…¥è¢«ä¼ é€’åˆ°æç¤ºæ¨¡æ¿ï¼Œç„¶åæç¤ºæ¨¡æ¿çš„è¾“å‡ºè¢«ä¼ é€’åˆ°æ¨¡å‹ï¼Œæ¥ç€æ¨¡å‹çš„è¾“å‡ºè¢«ä¼ é€’åˆ°è¾“å‡ºè§£æå™¨ã€‚*

#+begin_src python :eval no
  chain = prompt | model | output_parser
#+end_src

ç«–çº¿ç¬¦å·ç±»ä¼¼äºUnixç®¡é“æ“ä½œç¬¦ï¼Œå®ƒå°†ä¸åŒçš„ç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œå°†ä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªç»„ä»¶çš„è¾“å…¥ã€‚

** è½¯ä»¶ä¾èµ–ç‰ˆæœ¬
æœ¬æŒ‡å—ä½¿ç”¨çš„ LangChain åŠç›¸å…³è½¯ä»¶ç‰ˆæœ¬å¦‚ä¸‹ï¼š

#+begin_src shell :results output raw
  pip show langchain-core langchain-community langchain-openai
#+end_src

#+RESULTS:
Name: langchain-core
Version: 0.2.0
Summary: Building applications with LLMs through composability
Home-page: https://github.com/langchain-ai/langchain
Author: 
Author-email: 
License: MIT
Location: /Users/wangjian/.virtualenvs/jupyter/lib/python3.12/site-packages
Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity
Required-by: langchain, langchain-chroma, langchain-community, langchain-experimental, langchain-openai, langchain-text-splitters
---
Name: langchain-community
Version: 0.2.1
Summary: Community contributed LangChain integrations.
Home-page: https://github.com/langchain-ai/langchain
Author: 
Author-email: 
License: MIT
Location: /Users/wangjian/.virtualenvs/jupyter/lib/python3.12/site-packages
Requires: aiohttp, dataclasses-json, langchain, langchain-core, langsmith, numpy, PyYAML, requests, SQLAlchemy, tenacity
Required-by: langchain-experimental
---
Name: langchain-openai
Version: 0.1.7
Summary: An integration package connecting OpenAI and LangChain
Home-page: https://github.com/langchain-ai/langchain
Author: 
Author-email: 
License: MIT
Location: /Users/wangjian/.virtualenvs/jupyter/lib/python3.12/site-packages
Requires: langchain-core, openai, tiktoken
Required-by: 

#+begin_src jupyter-python
  from langchain_openai import ChatOpenAI
  from langchain_core.output_parsers import StrOutputParser
  from langchain_core.prompts import ChatPromptTemplate

  # åˆå§‹åŒ– ChatOpenAI æ¨¡å‹ï¼ŒæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ä¸º 'gpt-3.5-turbo-0125'
  model = ChatOpenAI(model="gpt-3.5-turbo-0125", base_url='https://api.xty.app/v1')

  # åˆ›å»ºä¸€ä¸ªèŠå¤©æç¤ºæ¨¡æ¿ï¼Œè®¾ç½®æ¨¡æ¿å†…å®¹ä¸º"è®²ä¸ªå…³äº {topic} çš„ç¬‘è¯å§"
  prompt = ChatPromptTemplate.from_template("è®²ä¸ªå…³äº {topic} çš„ç¬‘è¯å§")

  # åˆå§‹åŒ–è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²
  output_parser = StrOutputParser()

  # æ„å»ºä¸€ä¸ªå¤„ç†é“¾ï¼Œå…ˆé€šè¿‡æç¤ºæ¨¡æ¿ç”Ÿæˆå®Œæ•´çš„è¾“å…¥ï¼Œç„¶åé€šè¿‡æ¨¡å‹å¤„ç†ï¼Œæœ€åè§£æè¾“å‡º
  chain = prompt | model | output_parser

  # è°ƒç”¨å¤„ç†é“¾ï¼Œä¼ å…¥ä¸»é¢˜ä¸º"ç¨‹åºå‘˜"ï¼Œç”Ÿæˆå…³äºç¨‹åºå‘˜çš„ç¬‘è¯
  chain.invoke({"topic": "ç¨‹åºå‘˜"})
#+end_src

#+RESULTS:
: å½“ç„¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç»å…¸çš„ç¨‹åºå‘˜ç¬‘è¯ï¼š\n\nç¨‹åºå‘˜èµ°è¿›ä¸€å®¶é…’å§ï¼ŒæœåŠ¡å‘˜é—®é“ï¼šâ€œä½ æƒ³è¦å–ç‚¹ä»€ä¹ˆï¼Ÿâ€ \n\nç¨‹åºå‘˜å›ç­”è¯´ï¼šâ€œä¸€æ¯æ°´å§ã€‚â€ \n\næœåŠ¡å‘˜é—®ï¼šâ€œä¸ºä»€ä¹ˆä¸å–ç‚¹é…’ï¼Ÿâ€ \n\nç¨‹åºå‘˜ç¬‘ç€è¯´ï¼šâ€œå› ä¸ºå–é…’åæˆ‘å°±å˜æˆäº†ä¸€ä¸ª 'bug' ç¨‹åºå‘˜ï¼â€

* æ ¸å¿ƒæ¦‚å¿µï¼šinvoke æ–¹æ³•
Langchain =invoke= æ–¹æ³•æ˜¯ LCEL è®¾è®¡ä¸­çš„é‡è¦æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œç»“åˆè¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ„å»ºï¼Œå®ç°ä¸åŒæ•°æ®æºå’ŒAPIçš„æ¥å£å¯¹æ¥ã€‚

** åŸºç¡€æ¦‚å¿µ
- åœ¨Langchainä¸­ï¼Œinvokeæ–¹æ³•æ˜¯æ‰€æœ‰LangChainè¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼‰å¯¹è±¡çš„é€šç”¨åŒæ­¥è°ƒç”¨æ–¹æ³•ã€‚é€šè¿‡invokeæ–¹æ³•ï¼Œå¼€å‘è€…å¯ä»¥ç›´æ¥è°ƒç”¨LLMæˆ–ChatModelï¼Œç®€åŒ–äº†è°ƒç”¨æµç¨‹ï¼Œæé«˜äº†å¼€å‘æ•ˆç‡ã€‚
- ä¸å…¶ä»–é“¾å¼è°ƒç”¨æ–¹æ³•ç›¸æ¯”ï¼Œinvokeæ–¹æ³•æ›´åŠ çµæ´»ä¾¿æ·ï¼Œå¯ä»¥ç›´æ¥å¯¹è¾“å…¥è¿›è¡Œè°ƒç”¨ï¼Œè€Œä¸éœ€è¦é¢å¤–çš„é“¾å¼æ“ä½œã€‚ç›¸å¯¹äºbatchå’Œstreamç­‰å¼‚æ­¥æ–¹æ³•ï¼Œinvokeæ–¹æ³•æ›´é€‚ç”¨äºå•ä¸€æ“ä½œçš„æ‰§è¡Œã€‚

** ä½¿ç”¨æ–¹å¼
- å•æ¬¡è°ƒç”¨ï¼šé€šè¿‡invokeæ–¹æ³•ï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿå¯¹å•ä¸€æ“ä½œè¿›è¡Œæ‰§è¡Œï¼Œä¾‹å¦‚è½¬æ¢ChatMessageä¸ºPythonå­—ç¬¦ä¸²ç­‰ç®€å•æ“ä½œï¼Œæå‡äº†ä»£ç çš„å¯è¯»æ€§å’Œæ•´æ´åº¦ã€‚
- å¤æ‚åä½œï¼šLangchainçš„æ ¸å¿ƒç†å¿µå°±æ˜¯å°†è¯­è¨€æ¨¡å‹ä½œä¸ºåä½œå·¥å…·ï¼Œinvokeæ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°å®ç°å¼€å‘è€…ä¸è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆäº’åŠ¨ï¼Œæ„å»ºå‡ºå¤„ç†å¤æ‚ä»»åŠ¡çš„ç³»ç»Ÿï¼Œå¹¶å¯¹æ¥ä¸åŒçš„æ•°æ®æºå’ŒAPIæ¥å£ã€‚

* invoke ä¸ Model I/O çš„ç»“åˆ
æ•´ä¸ªæµç¨‹æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š

1. =Prompt= ç»„ä»¶æ¥æ”¶ç”¨æˆ·è¾“å…¥ *{"topic": "ç¨‹åºå‘˜"}*ï¼Œç„¶åä½¿ç”¨è¯¥ topic æ„å»º =PromptValue=
2. =Model= ç»„ä»¶è·å–ç”Ÿæˆçš„æç¤ºï¼Œå¹¶ä¼ é€’ç»™ GPT-3.5-Turbo æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ä»æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæ˜¯ä¸€ä¸ªChatMessageå¯¹è±¡ã€‚
3. æœ€åï¼Œ =output_parser= ç»„ä»¶æ¥æ”¶ChatMessageï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºPythonå­—ç¬¦ä¸²ï¼Œåœ¨invokeæ–¹æ³•ä¸­è¿”å›ã€‚

** Prompt
=prompt= æ˜¯ =BasePromptTemplate= çš„å®ä¾‹ï¼Œè¿™æ„å‘³ç€å®ƒæ¥å—æ¨¡æ¿å˜é‡çš„å­—å…¸å¹¶ç”Ÿæˆä¸€ä¸ª =PromptValue= ã€‚

PromptValueæ˜¯ä¸€ä¸ªåŒ…è£…å™¨(wrapper)ï¼Œå›´ç»•å®Œæˆçš„æç¤ºè¿›è¡Œæ“ä½œï¼Œå¯ä»¥ä¼ é€’ç»™LLMï¼ˆä»¥å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼‰æˆ–ChatModelï¼ˆä»¥æ¶ˆæ¯åºåˆ—ä½œä¸ºè¾“å…¥ï¼‰ã€‚

å®ƒå¯ä»¥ä¸ä»»ä½•è¯­è¨€æ¨¡å‹ç±»å‹ä¸€èµ·ä½¿ç”¨ï¼Œå› ä¸ºå®ƒå®šä¹‰äº†ç”Ÿæˆ=BaseMessages=å’Œç”Ÿæˆå­—ç¬¦ä¸²çš„é€»è¾‘ã€‚

#+begin_src python
  from langchain import PromptTemplate

  # Prompt é LCEL ä½¿ç”¨æ–¹æ³•
  prompt_template = PromptTemplate.from_template(
      "è®²ä¸ªå…³äº {topic} çš„ç¬‘è¯å§"
  )

  # ä½¿ç”¨ format ç”Ÿæˆæç¤º
  prompt = prompt_template.format(topic="ç¨‹åºå‘˜")
  print(prompt)
#+end_src

#+RESULTS:
: è®²ä¸ªå…³äº ç¨‹åºå‘˜ çš„ç¬‘è¯å§

#+begin_src jupyter-python
  # è°ƒç”¨ Prompt çš„ invoke æ–¹æ³•ç”Ÿæˆæœ€ç»ˆçš„æç¤ºè¯
  prompt_value = prompt.invoke({"topic": "ç¨‹åºå‘˜"})
  prompt_value
#+end_src

#+RESULTS:
: ChatPromptValue(messages=[HumanMessage(content='è®²ä¸ªå…³äº ç¨‹åºå‘˜ çš„ç¬‘è¯å§')])

** Model
ç„¶åè°ƒç”¨æ¨¡å‹çš„ =invoke= æ–¹æ³•ï¼Œå°† =PromptValue= ä¼ é€’ç»™æ¨¡å‹ã€‚

æˆ‘ä»¬ä½¿ç”¨çš„ =GPT-3.5-turbo= æ¨¡å‹æ˜¯ ChatModelï¼Œinvoke æ–¹æ³•å°†è¿”å› BaseMessage ã€‚

#+begin_src jupyter-python :results none
  message = model.invoke(prompt_value)
#+end_src

#+begin_src jupyter-python
  message
#+end_src

#+RESULTS:
: AIMessage(content="å½“ç„¶ï¼Œè¿™æ˜¯ä¸€ä¸ª did the programmer quit his job? Because he didn't get arrays of satisfaction!\n\nImagine a scenario: a group of programmers gathered around a table cluttered with laptops and empty coffee cups. One", response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 22, 'total_tokens': 66}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}, id='run-d500a9cc-ce12-45a9-a71a-56e494fb5509-0')

å¦‚æœæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ LLM æ¨¡å‹ =gpt-3.5-turbo-instruct= ï¼Œinvoke æ–¹æ³•å°±ä¼šè¿”å›å­—ç¬¦ä¸²ã€‚

#+begin_src jupyter-python
  from langchain_openai import OpenAI

  llm = OpenAI(model="gpt-3.5-turbo-instruct", base_url='https://api.xty.app/v1')
  llm.invoke(prompt_value)
#+end_src

#+RESULTS:
: ä¸ºä»€ä¹ˆç¨‹åºå‘˜å–œæ¬¢åœ¨æµ·æ»©ä¸Šæ•£æ­¥å‘¢ï¼Ÿ\n\nå› ä¸ºä»–ä»¬æƒ³çœ‹çœ‹shellè„šæœ¬çš„shellã€‚

** Output Parser
æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹è¾“å‡ºä¼ é€’ç»™ output_parserï¼Œå®ƒæ˜¯ä¸€ä¸ª =BaseOutputParser= ç¤ºä¾‹ï¼Œæ„å‘³ç€å®ƒæ¥å—å­—ç¬¦ä¸²æˆ– BaseMessage ä½œä¸ºè¾“å…¥ã€‚

æœ¬æŒ‡å—ä¸­ä½¿ç”¨çš„ =StrOutputParser= ç¤ºä¾‹å°†æ‰€æœ‰è¾“å…¥éƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚

#+begin_src jupyter-python
  # message ç»è¿‡ StrOutputParser å¤„ç†ï¼Œå˜ä¸ºæ ‡å‡†çš„å­—ç¬¦ä¸²
  output_parser.invoke(message)
#+end_src

#+RESULTS:
: å½“ç„¶ï¼Œè¿™æ˜¯ä¸€ä¸ª did the programmer quit his job? Because he didn't get arrays of satisfaction!\n\nImagine a scenario: a group of programmers gathered around a table cluttered with laptops and empty coffee cups. One

* Invoke ä¸ Retrieval ç»“åˆ
ä¸‹é¢æ¼”ç¤ºå¦‚ä½•åœ¨ç»å…¸çš„ RAG åœºæ™¯ä¸­ä½¿ç”¨ invoke æ–¹æ³•ã€‚ä¸‹é¢å°†ä½¿ç”¨ =|= æ“ä½œç¬¦å®ç°æ›´å¤æ‚çš„é“¾å¼è°ƒç”¨ã€‚

#+begin_src python
  chain = setup_and_retrieval | prompt | model | output_parser
#+end_src

ä¸ºäº†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆå¯ä»¥çœ‹åˆ°ä¸Šé¢çš„æç¤ºæ¨¡æ¿æ¥å—ä¸Šä¸‹æ–‡å’Œé—®é¢˜ä½œä¸ºè¦æ›¿æ¢åœ¨æç¤ºä¸­çš„å€¼ã€‚åœ¨æ„å»ºæç¤ºæ¨¡æ¿ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›æ£€ç´¢ç›¸å…³æ–‡ä»¶ä»¥åŠå°†å®ƒä»¬åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚

ä½œä¸ºåˆæ­¥æ­¥éª¤ï¼Œæˆ‘ä»¬å·²ç»è®¾ç½®äº†ä½¿ç”¨å†…å­˜å­˜å‚¨å™¨çš„æ£€ç´¢å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®æŸ¥è¯¢æ£€ç´¢æ–‡æ¡£ã€‚è¿™ä¹Ÿæ˜¯ä¸€ä¸ªå¯è¿è¡Œçš„ç»„ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–ç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥å°è¯•å•ç‹¬è¿è¡Œå®ƒï¼š

æ•´ä¸ªæµç¨‹å¦‚ä¸‹ï¼š

1. é¦–å…ˆåˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ¡ç›®(entries)çš„ =RunnableParallel= å¯¹è±¡ *setup_and_retrieval* ã€‚
   ç¬¬ä¸€ä¸ªæ¡ç›® =context= å°†åŒ…æ‹¬æ£€ç´¢å™¨è·å–çš„æ–‡æ¡£ç»“æœã€‚
   ç¬¬äºŒä¸ªæ¡ç›® =question= å°†åŒ…å«ç”¨æˆ·åŸå§‹é—®é¢˜ã€‚
   ä¸ºäº†ä¼ é€’é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ =RunnablePassthrough= æ¥å¤åˆ¶è¿™ä¸ªæ¡ç›®ã€‚
2. å°†ä¸Šä¸€æ­¥ä¸­çš„å­—å…¸æä¾›ç»™ =Prompt= ç»„ä»¶ã€‚ç„¶åï¼Œå®ƒæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼ˆå³é—®é¢˜ï¼‰ä»¥åŠæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆå³contextï¼‰ï¼Œæ„å»ºæç¤ºå¹¶è¾“å‡º =PromptValue= ã€‚
3. =Model= ç»„ä»¶æ¥å—ç”Ÿæˆçš„æç¤ºï¼Œå¹¶ä¼ é€’ç»™OpenAI =gpt-3.5-turbo-0125= æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæ˜¯ä¸€ä¸ªChatMessageå¯¹è±¡ã€‚
4. æœ€åï¼Œ =output_parser= ç»„ä»¶æ¥æ”¶ChatMessageï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºPythonå­—ç¬¦ä¸²ï¼Œåœ¨è°ƒç”¨æ–¹æ³•ä¸­è¿”å›è¯¥å­—ç¬¦ä¸²ã€‚

** è½¯ä»¶ä¾èµ–ç‰ˆæœ¬
æœ¬æŒ‡å—ä½¿ç”¨çš„ LangChain åŠç›¸å…³è½¯ä»¶ç‰ˆæœ¬å¦‚ä¸‹ï¼š

#+begin_src shell :results raw
  pip show langchain docarray tiktoken
#+end_src

#+RESULTS:
Name: langchain
Version: 0.2.0
Summary: Building applications with LLMs through composability
Home-page: https://github.com/langchain-ai/langchain
Author: 
Author-email: 
License: MIT
Location: /Users/wangjian/.virtualenvs/jupyter/lib/python3.12/site-packages
Requires: aiohttp, dataclasses-json, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity
Required-by: langchain-community
---
Name: tiktoken
Version: 0.7.0
Summary: tiktoken is a fast BPE tokeniser for use with OpenAI's models
Home-page: 
Author: Shantanu Jain
Author-email: shantanu@openai.com
License: MIT License

Copyright (c) 2022 OpenAI, Shantanu Jain

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

Location: /Users/wangjian/.virtualenvs/jupyter/lib/python3.12/site-packages
Requires: regex, requests
Required-by: langchain-openai

#+begin_src jupyter-python :results none
  from langchain_openai import ChatOpenAI

  model = ChatOpenAI(model="gpt-3.5-turbo-0125", base_url='https://api.xty.app/v1')
#+end_src

#+begin_src jupyter-python
  # å¯¼å…¥ LangChain åº“çš„ä¸åŒæ¨¡å—ï¼ŒåŒ…æ‹¬å‘é‡å­˜å‚¨ã€è¾“å‡ºè§£æå™¨ã€æç¤ºæ¨¡æ¿ã€å¹¶è¡Œè¿è¡Œå™¨å’Œ OpenAI çš„åµŒå…¥æ¨¡å‹
  from langchain_community.vectorstores import DocArrayInMemorySearch
  from langchain_core.output_parsers import StrOutputParser
  from langchain_core.prompts import ChatPromptTemplate
  from langchain_core.runnables import RunnableParallel, RunnablePassthrough
  from langchain_openai import OpenAIEmbeddings

  # ä½¿ç”¨ DocArrayInMemorySearch åˆ›å»ºä¸€ä¸ªå†…å­˜ä¸­çš„å‘é‡å­˜å‚¨
  # ä½¿ç”¨ OpenAIEmbeddings ä¸ºæ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡ï¼Œæ–‡æœ¬ä¸º "harrison worked at kensho" å’Œ "bears like to eat honey"
  vectorstore = DocArrayInMemorySearch.from_texts(
      ["harrison worked at kensho", "bears like to eat honey"],
      embedding=OpenAIEmbeddings(base_url='https://api.xty.app/v1'),
  )

  # å°†å‘é‡å­˜å‚¨è½¬æ¢ä¸ºæ£€ç´¢å™¨
  retriever = vectorstore.as_retriever()

  # åˆ›å»ºä¸€ä¸ªèŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨ä¸­æ–‡è®¾ç½®æ¨¡æ¿ä»¥ä¾¿ç”ŸæˆåŸºäºç‰¹å®šä¸Šä¸‹æ–‡å’Œé—®é¢˜çš„å®Œæ•´è¾“å…¥
  template = """æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:
  {context}

  é—®é¢˜: {question}
  """
  prompt = ChatPromptTemplate.from_template(template)

  # åˆå§‹åŒ–è¾“å‡ºè§£æå™¨ï¼Œå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²
  output_parser = StrOutputParser()

  # è®¾ç½®ä¸€ä¸ªå¹¶è¡Œè¿è¡Œå™¨ï¼Œç”¨äºåŒæ—¶å¤„ç†ä¸Šä¸‹æ–‡æ£€ç´¢å’Œé—®é¢˜ä¼ é€’
  # ä½¿ç”¨RunnableParallelæ¥å‡†å¤‡é¢„æœŸçš„è¾“å…¥ï¼Œé€šè¿‡ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£æ¡ç›®ä»¥åŠåŸå§‹ç”¨æˆ·é—®é¢˜ï¼Œ
  # åˆ©ç”¨æ–‡æ¡£æœç´¢å™¨ retriever è¿›è¡Œæ–‡æ¡£æœç´¢ï¼Œå¹¶ä½¿ç”¨ RunnablePassthrough æ¥ä¼ é€’ç”¨æˆ·çš„é—®é¢˜ã€‚
  setup_and_retrieval = RunnableParallel(
      {"context": retriever, "question": RunnablePassthrough()}
  )

  # æ„å»ºä¸€ä¸ªå¤„ç†é“¾ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡å’Œé—®é¢˜çš„è®¾ç½®ã€æç¤ºç”Ÿæˆã€æ¨¡å‹è°ƒç”¨å’Œè¾“å‡ºè§£æ
  chain = setup_and_retrieval | prompt | model | output_parser

  # è°ƒç”¨å¤„ç†é“¾ï¼Œä¼ å…¥é—®é¢˜"where did harrison work?"ï¼ˆéœ€ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰ï¼Œå¹¶åŸºäºç»™å®šçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
  chain.invoke("harrisonåœ¨å“ªé‡Œå·¥ä½œï¼Ÿ")
#+end_src

#+RESULTS:
:RESULTS:
: /Users/wangjian/.virtualenvs/jupyter/lib/python3.12/site-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.
:   warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')
: Harrisonåœ¨Kenshoå·¥ä½œã€‚
:END:
